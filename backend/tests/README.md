# Test Suite for MEWallet Backend

This directory contains comprehensive test cases for all MEWallet API endpoints.

## ğŸ“ Test Structure

```
tests/
â”œâ”€â”€ __init__.py                    # Package marker
â”œâ”€â”€ conftest.py                    # Shared fixtures and setup
â”œâ”€â”€ test_user_routes.py            # User authentication & profile tests
â”œâ”€â”€ test_merchant_routes.py        # Merchant authentication & profile tests
â”œâ”€â”€ test_transaction_routes.py     # Transaction & balance tests
â”œâ”€â”€ test_pay_request_routes.py     # Pay request workflow tests
â””â”€â”€ test_oauth_routes.py           # OAuth authentication tests
```

## ğŸ§ª Test Coverage

### User Routes (`test_user_routes.py`)
- âœ… User registration (success, validation errors)
- âœ… User login (success, invalid credentials, missing fields)
- âœ… User profile retrieval (authorized, unauthorized)
- âœ… Linked merchants listing

### Merchant Routes (`test_merchant_routes.py`)
- âœ… Merchant registration (success, duplicate phone, validation)
- âœ… Merchant login (success, invalid credentials)
- âœ… Merchant profile retrieval
- âœ… Linked users listing

### Transaction Routes (`test_transaction_routes.py`)
- âœ… Create merchant-user link (success, invalid PIN, duplicates)
- âœ… Add balance (success, invalid PIN, negative amounts)
- âœ… Process purchase (success, insufficient balance, wrong PIN)
- âœ… Get balance
- âœ… Get transactions

### Pay Request Routes (`test_pay_request_routes.py`)
- âœ… Create pay request (success, no link, negative amount)
- âœ… Get user/merchant pay requests
- âœ… Accept pay request (success, invalid PIN, insufficient balance)
- âœ… Reject pay request

### OAuth Routes (`test_oauth_routes.py`)
- âœ… Google OAuth login/registration (mocked)
- âœ… Complete merchant profile
- âœ… Complete user profile
- âœ… Invalid token handling

## ğŸš€ Running Tests

### Run All Tests
```bash
pytest
```

### Run Specific Test File
```bash
pytest tests/test_user_routes.py
pytest tests/test_transaction_routes.py
```

### Run Specific Test Class
```bash
pytest tests/test_user_routes.py::TestUserRegistration
```

### Run Specific Test Function
```bash
pytest tests/test_user_routes.py::TestUserRegistration::test_register_user_success
```

### Run with Coverage Report
```bash
pytest --cov=. --cov-report=html
```
Then open `htmlcov/index.html` in your browser.

### Run with Verbose Output
```bash
pytest -v
```

### Run Only Fast Tests (Skip Slow Tests)
```bash
pytest -m "not slow"
```

## ğŸ“Š Test Fixtures

### Available Fixtures (from `conftest.py`)

- **`client`**: TestClient for making API requests
- **`supabase`**: Supabase database client
- **`test_user`**: Creates a test user with cleanup
- **`test_merchant`**: Creates a test merchant with cleanup
- **`user_token`**: JWT token for test user
- **`merchant_token`**: JWT token for test merchant
- **`test_link`**: Creates merchant-user link with PIN

### Usage Example
```python
def test_example(client, test_user, user_token):
    headers = {"Authorization": f"Bearer {user_token}"}
    response = client.get(f"/user/profile/{test_user['user_id']}", headers=headers)
    assert response.status_code == 200
```

## ğŸ”§ Requirements

Install test dependencies:
```bash
pip install pytest pytest-cov faker httpx
```

Or if you have a requirements.txt:
```bash
pip install -r requirements.txt
```

## ğŸ“ Writing New Tests

### Test Class Naming Convention
```python
class TestFeatureName:
    """Test description"""
    
    def test_scenario_success(self, fixtures):
        """Test successful scenario"""
        pass
    
    def test_scenario_failure(self, fixtures):
        """Test failure scenario"""
        pass
```

### Test Function Naming
- `test_<feature>_success` - Happy path
- `test_<feature>_<error_type>` - Error cases
- `test_<feature>_unauthorized` - Auth failures
- `test_<feature>_validation` - Input validation

### Common Assertions
```python
# Status codes
assert response.status_code == 200
assert response.status_code == 401

# Response data
assert "key" in response.json()
assert response.json()["message"] == "Success"
assert isinstance(response.json(), list)

# Error messages
assert "error text" in response.json()["detail"]
```

## ğŸ› Debugging Failed Tests

### Run with Detailed Output
```bash
pytest -vv --tb=long
```

### Run Last Failed Tests Only
```bash
pytest --lf
```

### Run with PDB Debugger
```bash
pytest --pdb
```

### Print Output During Tests
```bash
pytest -s
```

## ğŸ“ˆ Coverage Goals

Target coverage: **> 80%**

Current coverage by module:
- User Routes: ~90%
- Merchant Routes: ~90%
- Transaction Routes: ~85%
- Pay Request Routes: ~85%
- OAuth Routes: ~75% (requires mocking)

## ğŸ” Security Considerations

- All fixtures automatically clean up test data
- Test database should be separate from production
- Sensitive data (passwords, tokens) are generated via faker
- OAuth tests use mocked Google verification

## ğŸ“ Support

For issues or questions about tests:
1. Check test output for error details
2. Verify database connection in `.env`
3. Ensure all dependencies are installed
4. Check fixture cleanup in `conftest.py`

## ğŸ¯ Best Practices

1. **Isolation**: Each test should be independent
2. **Cleanup**: Use fixtures for automatic cleanup
3. **Mocking**: Mock external services (Google OAuth)
4. **Assertions**: Use descriptive assertion messages
5. **Coverage**: Aim for both success and failure paths
6. **Documentation**: Add docstrings to test functions

## ğŸ”„ Continuous Integration

To run tests in CI/CD:
```bash
# Install dependencies
pip install -r requirements.txt

# Run tests with coverage
pytest --cov=. --cov-report=xml

# Check coverage threshold
coverage report --fail-under=80
```
